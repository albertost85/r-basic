---
output:
  pdf_document: default
  html_document: default
---
## Introducción a la regresión lineal

Tenemos una serie de puntos en el plano cartesiano $\mathbb{R}^2$, de la forma
$$(x_1,y_1),\ \dots,\ (x_n,y_n)$$
Se trata de obtener cual es la correlación $y=f(x)$ que mejor aproxime y en función de x.

## Modelo recta
Ee busca la recta $y = ax +b$ que mejor aproxime los puntos dados imponiendo que la suma de los cuadrados de las diferencias entre los valores $y_i$ y sus aproximaciones $\tilde{y}_i=ax_i+b$ sea mínima:
$$err = \sum_{i=1}^n(y_i-\tilde{y}_i)^2$$ sea mínima

## Ejemplo
Trabajaremos con las variables `fat` y `weight`.
```{r}
body = read.table("data/bodyfat.txt", header = TRUE)
body2 = body[,c(2,4)]
names(body2) = c("Grasa","Peso")
str(body2)
plot(body2)
```


Si organizamos todos los datos en un único dataframes en vez de en dos vectores o por separado, la organización de datos es más cómoda. Si los vectores `y` y `x` son, en este orden, la primera y la segunda columna de un data frame de dos variables, entonces es suficiente aplicar la función `lm` al data frame. 

En general, si `x` e `y` son dos variables de un data frame, para calcular la recta de regresión de `y` en función de `x` podemos usar la instrucción `lm(y~x, data = data fame)` La tilde significa y en función de x.



```{r}
lm(body2$Peso~body2$Grasa) #Opción 1
lm(Peso~Grasa, data = body2) #Opción 2
```
El resultado obtenido  significa que la recta de regresión para nuestros datos es 
$$y = 2.151x+137.738$$

## Representar la recta de regresión

```{r}
plot(body2)
abline(lm(Peso~Grasa, data = body2), col = "purple")
```

## Observación

Hay que tener en cuenta que el análisis llevado a cabo hasta el momento de los pares de valores $(x_i,y_i)_{i=1,\dots,n}$ ha sido puramente descriptivo.

Es decir, hemos mostrado que estos datos son consistentes con una función lineal, pero no hemos demostrado que la variable dependiente sea función aproximadamente lineal de la variable dependiente. Esto último necesitaría una demostración matemática, o bien un argumento biológico, pero no basta con una simple comprobación numérica.

## Haciendo predicciones

Eso sí, podemos utilizar todo lo hecho hasta ahora para predecir valores $\tilde{y}_i$ en función de los $x_i$  resolviendo una simple ecuación lineal


## Coeficiente de determinación

El <l class = "definition">coeficiente de determinación</l>, $R^2$, nos es útil para evaluar numéricamente si la relación lineal obtenida es significativa o no.

No explicaremos de momento como se define. Eso lo dejamos para curiosidad del usuario. Por el momento, es suficiente con saber que este coeficiente se encuentra en el intervalo $[0,1]$. Si $R^2$ es mayor a 0.9, consideraremos que el ajuste es bueno. De lo contrario, no.

## La función summary

La función `summary` aplicada a `lm` nos muestra los contenidos de este objeto. Entre ellos encontramos `Multiple R-squared`, que no es ni más ni menos que el coeficiente de determinación, $R^2$.

Para facilitarnos las cosas y ahorrarnos información que, de momento, no nos resulta de interés, podemos aplicar `summary(lm(...))$r.squared`

## Ejemplo 1

```{r}
summary(lm(Peso~Grasa, data = body2))
```

## Ejemplo 1

```{r}
summary(lm(Peso~Grasa, data = body2))$r.squared
```

<div class = "example">
En este caso, hemos obtenido un coeficiente de determinación de 0.3751, cosa que confirma que la recta de regresión no aproxima nada bien nuestros datos. 
</div>

# Rectas de regresión y transformaciones logarítmicas

## Transformaciones logarítmicas

No siempre encontraremos dependencias lineales. A veces nos encontraremos otro tipo de dependencias, como por ejemplo pontencias o exponenciales.

Estas se pueden transformar a lineales mediante un <l class = "definition">cambio de escala</l>

## Escalas logarítmicas

Por lo general, es habitual encontrarnos gráficos con sus ejes en <l class = "definition">escala lineal</l>. Es decir, las marcas en los ejes están igualmente espaciadas.

A veces, es conveniente dibujar alguno de los ejes en <l class = "definition">escala logarítmica</l>, de modo que la misma distancia entre las marcas significa el mismo cociente entre sus valores. En otras palabras, un eje en escala logarítmica representa el [logaritmo](https://es.wikipedia.org/wiki/Logaritmo) de sus valores en escala lineal.

Diremos que un gráfico está en <l class = "definition">escala semilogarítmica</l> cuando su eje de abcisas está en escala lineal y, el de ordenadas, en escala logarítmica.

Diremos que un gráfico está en <l class = "definition">escala doble logarítmica</l> cuando ambos ejes están en escala logarítmica.

## Interpretación gráfica

Si al representar unos puntos $(x_i, y_i)_{i=1,\dots,n}$ en escala semilogarítmica observamos que siguen aproximadamente una recta, esto querrá decir que los valores $\log(y)$ siguen una ley aproximadamente lineal en los valores $x$, y, por lo tanto, que $y$ sigue una <l class = "definition">ley aproximadamente exponencial</l> en $x$. 

En efecto, si $\log(y) = ax + b$, entonces,

$$y = 10^{\log(y)} = 10^{ax+b} = 10^{ax}\cdot 10^b = \alpha^x\beta$$

con $\alpha = 10^a$ y $\beta = 10^b$

## Interpretación gráfica

Si al representar unos puntos $(x_i, y_i)_{i=1,\dots,n}$ en escala doble logarítmica observamos que siguen aproximadamente una recta, esto querrá decir que los valores $\log(y)$ siguen una ley aproximadamente lineal en los valores $\log(x)$, y, por lo tanto, que $y$ sigue una <l class = "definition">ley aproximadamente potencial</l> en $x$. 

En efecto, si $\log(y) = a\log(x) + b$, entonces, por propiedades de logaritmos

$$y = 10^{\log(y)} = 10^{a\log(x)+b}= (10^{\log(x)})^a\cdot 10^b = x^{a}\beta$$
con $\beta = 10^b$

## Ejemplo 2

<div class = "example">
**Ejemplo 2**

En este caso trabajaremos no con un data frame, sino directamente con los dos vectores siguientes: 

</div>

```{r}
dep = c(1.2,3.6,12,36)
ind = c(20,35,61,82)
```

```{r, eval = FALSE}
plot(ind,dep, main = "Escala lineal")
plot(ind,dep, log = "y", main = "Escala semilogarítmica")
```

## Ejemplo 2

```{r, echo = FALSE}
par(mfrow = c(1,2))
plot(ind,dep, main = "Escala lineal")
plot(ind,dep, log = "y", main = "Escala semilogarítmica")
par(mfrow = c(1,1))
```

## Ejemplo 2

```{r}
lm(log10(dep)~ind)
summary(lm(log10(dep)~ind))$r.squared
```

## Ejemplo 2{.example}

Lo que acabamos de obtener es que 

$$\log(dep) = 0.023\cdot ind - 0.33$$
es una buena aproximación de nuestros datos.

Con lo cual

$$dep = 10^{0.023\cdot ind}\cdot10^{-0.33} = `r round(10^(0.023),3)`^{ind}\cdot `r round(10^(-0.33),3)`$$

## Ejemplo 2

```{r}
plot(ind,dep, main = "Curva de regresión")
curve(1.054^x*0.468, add = TRUE, col = "purple") 
```

## Ejemplo 3

<div class = "example">
**Ejemplo 3**

En este caso trabajaremos con el siguiente data frame:

</div>

```{r}
tiempo = 1:10
gramos = c(0.097,0.709,2.698,6.928,15.242,29.944,52.902,83.903,120.612,161.711)
d.f = data.frame(tiempo,gramos)
```

```{r,eval = FALSE}
plot(d.f)
plot(d.f, log = "y")
plot(d.f, log = "xy")
```


## Ejemplo 3

```{r, echo = FALSE,fig.width=10}
par(mfrow= c(1,3))
plot(d.f)
plot(d.f, log = "y")
plot(d.f, log = "xy")
par(mfrow= c(1,1))
```


## Ejemplo 3

```{r}
lm(log10(gramos)~log10(tiempo), data = d.f)
summary(lm(log10(gramos)~log10(tiempo), data = d.f))$r.squared
```

## Ejemplo 3{.example}

Lo que acabamos de obtener es que 

$$\log(gramos) = 3.298\cdot \log(tiempo) - 1.093$$
es una buena aproximación de nuestros datos.

Con lo cual

$$gramos = 10^{3.298\cdot\log(tiempo)}\cdot10^{-1.093} = tiempo^{3.298}\cdot `r round(10^(-1.093),3)`$$

## Ejemplo 3

```{r}
plot(d.f, main = "Curva de regresión")
curve(x^(3.298)*0.081, add=TRUE, col = "purple")
```

