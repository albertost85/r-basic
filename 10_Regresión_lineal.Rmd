---
output:
  pdf_document: default
  html_document: default
---
## Introducción a la regresión lineal

Tenemos una serie de puntos en el plano cartesiano $\mathbb{R}^2$, de la forma
$$(x_1,y_1),\ \dots,\ (x_n,y_n)$$
Se trata de obtener cual es la correlación $y=f(x)$ que mejor aproxime y en función de x.

## Modelo recta
Ee busca la recta $y = ax +b$ que mejor aproxime los puntos dados imponiendo que la suma de los cuadrados de las diferencias entre los valores $y_i$ y sus aproximaciones $\tilde{y}_i=ax_i+b$ sea mínima:
$$err = \sum_{i=1}^n(y_i-\tilde{y}_i)^2$$ sea mínima

## Ejemplo
Trabajaremos con las variables `fat` y `weight`.
```{r}
body = read.table("data/bodyfat.txt", header = TRUE)
body2 = body[,c(2,4)]
names(body2) = c("Grasa","Peso")
str(body2)
plot(body2)
```


Si organizamos todos los datos en un único dataframes en vez de en dos vectores o por separado, la organización de datos es más cómoda. Si los vectores `y` y `x` son, en este orden, la primera y la segunda columna de un data frame de dos variables, entonces es suficiente aplicar la función `lm` al data frame. 

En general, si `x` e `y` son dos variables de un data frame, para calcular la recta de regresión de `y` en función de `x` podemos usar la instrucción `lm(y~x, data = data fame)` La tilde significa y en función de x.



```{r}
lm(body2$Peso~body2$Grasa) #Opción 1
lm(Peso~Grasa, data = body2) #Opción 2
```
El resultado obtenido  significa que la recta de regresión para nuestros datos es 
$$y = 2.151x+137.738$$

## Representar la recta de regresión

```{r}
plot(body2)
abline(lm(Peso~Grasa, data = body2), col = "purple")
```

## Evaluar el ajuste

El coeficiente de determinación, $R^2$, nos es útil para evaluar numéricamente si la relación lineal obtenida es significativa o no.
- Si $R^2$ es mayor a 0.9, consideraremos que el ajuste es bueno. De lo contrario, no.

La función `summary` aplicada a `lm` nos muestra los contenidos de este objeto. Podemos aplicar `summary(lm(...))$r.squared`, que devuelve el valor de "Multiple R-squared".
```{r}
summary(lm(Peso~Grasa, data = body2))$r.squared
```
En este caso, hemos obtenido un coeficiente de determinación de 0.3751, cosa que confirma que la recta de regresión no aproxima nada bien nuestros datos. 

Residuals da los errores entre la recta de regresión y los datos. 


## Curva de regresión logarítmica
Si al representar unos puntos $(x_i, y_i)_{i=1,\dots,n}$ en escala semilogarítmica observamos que siguen aproximadamente una recta, esto querrá decir que los valores $\log(y)$ siguen una ley aproximadamente lineal en los valores $x$, y, por lo tanto, que $y$ sigue una <l class = "definition">ley aproximadamente exponencial</l> en $x$. 

En efecto, si $\log(y) = ax + b$, entonces,

$$y = 10^{\log(y)} = 10^{ax+b} = 10^{ax}\cdot 10^b = \alpha^x\beta$$

con $\alpha = 10^a$ y $\beta = 10^b$

Diremos que un gráfico está en escala semilogarítmica cuando su eje de abcisas está en escala lineal y, el de ordenadas, en escala logarítmica.

Si al representar unos puntos $(x_i, y_i)_{i=1,\dots,n}$ en escala doble logarítmica observamos que siguen aproximadamente una recta, esto querrá decir que los valores $\log(y)$ siguen una ley aproximadamente lineal en los valores $\log(x)$, y, por lo tanto, que $y$ sigue una <l class = "definition">ley aproximadamente potencial</l> en $x$. 

En efecto, si $\log(y) = a\log(x) + b$, entonces, por propiedades de logaritmos

$$y = 10^{\log(y)} = 10^{a\log(x)+b}= (10^{\log(x)})^a\cdot 10^b = x^{a}\beta$$
con $\beta = 10^b$

Diremos que un gráfico está en escala doble logarítmica cuando ambos ejes están en escala logarítmica.


```{r}
dep = c(1.2,3.6,12,36)
ind = c(20,35,61,82)
```

Los dos plots muestran que los datos sólo se ajustan a una recta en la representación semilogarítmica. 
```{r, echo = FALSE}
par(mfrow = c(1,2))
plot(ind,dep, main = "Escala lineal")
plot(ind,dep, log = "y", main = "Escala semilogarítmica")
par(mfrow = c(1,1))
```

Se hace el ajuste tomando como variable dependiente el logaritmo en base 10 de la variable dependiente original. El ajuste muetra un R^2 superior a 9,99, mostrando que es un buen ajuste.
```{r}
lm(log10(dep)~ind)
summary(lm(log10(dep)~ind))$r.squared
```
Lo que acabamos de obtener es es una buena aproximación de nuestros datos a la fórmula: 

$$\log(y) = 0.023\cdot x - 0.33$$


Con lo cual

$$y = 10^{0.023\cdot x}\cdot10^{-0.33}$$


```{r}
plot(ind,dep, main = "Curva de regresión")
curve(1.054^x*0.468, add = TRUE, col = "purple") 
```

## Ejemplo 3
En este caso trabajaremos con el siguiente data frame, y vemos que la escala potencial es la que ajusta mejor los datos a una recta.
```{r}
tiempo = 1:10
gramos = c(0.097,0.709,2.698,6.928,15.242,29.944,52.902,83.903,120.612,161.711)
d.f = data.frame(tiempo,gramos)
par(mfrow= c(1,3))
plot(d.f, main="nolog")
plot(d.f, log = "y", main="semilog")
plot(d.f, log = "xy", main="log")
par(mfrow= c(1,1))
```


El ajuste a una recta se hace entre el logaritmo de la variable dependiente (y) en función del logaritmo de la variable independiente (x), con un ajuste mayor a 0,99.

```{r}
lm(log10(gramos)~log10(tiempo), data = d.f)
summary(lm(log10(gramos)~log10(tiempo), data = d.f))$r.squared
```


$$\log(y) = 3.298\cdot \log(x) - 1.093$$

Con lo cual

$$y = 10^{3.298\cdot\log(x)}\cdot10^{-1.093} $$
```{r}
plot(d.f, main = "Curva de regresión")
curve(x^(3.298)*0.081, add=TRUE, col = "purple")
```

